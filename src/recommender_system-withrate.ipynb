{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spark location\n",
    "\n",
    "findspark.init('/home/chunyi/spark-2.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('rec').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                              IntegerType, StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripSpace(string):\n",
    "    return string.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "\n",
    "data = spark.read.csv('../data/all_play_simple_withrate.log.fn', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "udf_stripSpace = udf(stripSpace, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all columns\n",
    "\n",
    "data = data.withColumn('user_id', udf_stripSpace(data['_c0']))\n",
    "data = data.withColumn('song_id', udf_stripSpace(data['_c1']))\n",
    "data = data.withColumn('play_times', udf_stripSpace(data['_c2']))\n",
    "#data = data.withColumn('song_length', udf_stripSpace(data['_c3']))\n",
    "#data = data.withColumn('paid', udf_stripSpace(data['_c4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select(data['user_id'].cast('int'),\n",
    "                  data['song_id'].cast('int'),\n",
    "                  data['play_times'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out null values \n",
    "\n",
    "data = data.filter('user_id > 0 and song_id > 0 and play_times >= 0 ')\n",
    "#data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check how many songs each user played, the purpose is to check wheather active user or not\n",
    "data.createOrReplaceTempView('data')\n",
    "user_song_count = spark.sql(\"SELECT user_id, count(song_id) as Played_song FROM data GROUP BY user_id\")\n",
    "#user_song_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o296.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4.0 (TID 10, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0a5f200d331e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_song_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o296.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4.0 (TID 10, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n"
     ]
    }
   ],
   "source": [
    "user_song_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o341.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 7.0 failed 1 times, most recent failure: Lost task 3.0 in stage 7.0 (TID 15, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-155f58b4722a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minactive_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_song_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count(song_id) < 4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minactive_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o341.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 7.0 failed 1 times, most recent failure: Lost task 3.0 in stage 7.0 (TID 15, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/chunyi/spark-2.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-21-266bebfcf27b>\", line 2, in stripSpace\nAttributeError: 'NoneType' object has no attribute 'replace'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:518)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\n"
     ]
    }
   ],
   "source": [
    "# make a dataframe of inactive users\n",
    "\n",
    "inactive_user = user_song_count.filter('count(song_id) < 4')\n",
    "inactive_user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+-----------+----+\n",
      "|  user_id| song_id|play_length|song_length|paid|\n",
      "+---------+--------+-----------+-----------+----+\n",
      "|154422682|20870993|      22013|        332|   0|\n",
      "|154421907| 6560858|         96|        161|   0|\n",
      "|154422630| 3385963|     235868|        235|   0|\n",
      "|154410267| 6777172|        164|        237|   0|\n",
      "|154422626| 3198036|     275249|          0|   0|\n",
      "+---------+--------+-----------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| song_id|user_count|\n",
      "+--------+----------+\n",
      "|20870989|      1689|\n",
      "|20870988|      1628|\n",
      "|20870987|      1367|\n",
      "|20870990|      1302|\n",
      "|20870991|       973|\n",
      "|20862199|       844|\n",
      "|15249349|       687|\n",
      "|20870992|       665|\n",
      "|20870993|       521|\n",
      "| 9950164|       517|\n",
      "|15807836|       387|\n",
      "| 6468891|       348|\n",
      "| 5237384|       280|\n",
      "|20868396|       254|\n",
      "|20804585|       221|\n",
      "|  466227|       214|\n",
      "| 5114569|       193|\n",
      "| 6411519|       174|\n",
      "| 7176392|       150|\n",
      "| 6749207|       139|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the most popular songs\n",
    "popular_songs = spark.sql(\"SELECT song_id, count(user_id) as user_count FROM data WHERE play_length*2 > song_length GROUP BY song_id ORDER BY user_count DESC\")\n",
    "popular_songs.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+-----------+----+\n",
      "|  user_id| song_id|play_length|song_length|paid|\n",
      "+---------+--------+-----------+-----------+----+\n",
      "|154422682|20870993|      22013|        332|   0|\n",
      "|154421907| 6560858|         96|        161|   0|\n",
      "|154422630| 3385963|     235868|        235|   0|\n",
      "|154410267| 6777172|        164|        237|   0|\n",
      "|154422626| 3198036|     275249|          0|   0|\n",
      "+---------+--------+-----------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use max song length to replace original song length\n",
    "max_song_length_matrix.createOrReplaceTempView('max_song_length_matrix')\n",
    "data = spark.sql(\"SELECT data.user_id, data.song_id, data.play_length,  max_song_length_matrix.max_length AS song_length FROM data LEFT JOIN max_song_length_matrix ON data.song_id = max_song_length_matrix.song_id\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+-----------------+\n",
      "|summary|             user_id|          song_id|       play_length|      song_length|\n",
      "+-------+--------------------+-----------------+------------------+-----------------+\n",
      "|  count|               93447|            93447|             93447|            93447|\n",
      "|   mean|1.4542572819025758E8|7937701.708690488|15438.451860412855|270.7987950388991|\n",
      "| stddev|3.3252280201555815E7|7264628.432150167| 63577.29571119192|333.8384470287966|\n",
      "|    min|              168955|             1672|                 0|                0|\n",
      "|    max|           154465966|         20876814|           2519470|             8946|\n",
      "+-------+--------------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round, sum, avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many times using play_length and song_length column\n",
    "data = data.withColumn('play_times', when(col('song_length') == 0, 0).otherwise(round(col('play_length')/col('song_length'))))\n",
    "\n",
    "#data.describe().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column to int type\n",
    "data = data.select(data['user_id'],\n",
    "                  data['song_id'],\n",
    "                  data['play_length'],\n",
    "                  data['song_length'],\n",
    "                  data['play_times'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|             user_id|          song_id|       play_length|      song_length|        play_times|\n",
      "+-------+--------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|               93447|            93447|             93447|            93447|             93447|\n",
      "|   mean|1.4542572819025758E8|7937701.708690488|15438.451860412855|270.7987950388991| 52.17460164585273|\n",
      "| stddev|3.3252280201555815E7|7264628.432150167| 63577.29571119192|333.8384470287966|217.95430310006304|\n",
      "|    min|              168955|             1672|                 0|                0|                 0|\n",
      "|    max|           154465966|         20876814|           2519470|             8946|              1191|\n",
      "+-------+--------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each user, calculate how many times each song plays\n",
    "\n",
    "data = spark.sql(\"SELECT user_id, song_id,  sum(play_times) AS total_play_times, sum(play_length) AS total_play_length, max(song_length) AS song_length FROM data GROUP BY user_id, song_id\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+-----------------+-----------+\n",
      "|  user_id| song_id|total_play_times|total_play_length|song_length|\n",
      "+---------+--------+----------------+-----------------+-----------+\n",
      "|154420326|15208411|               0|               22|         59|\n",
      "|154415167|  707143|               1|              228|        228|\n",
      "|154420350| 6560373|               0|                1|        173|\n",
      "|154422354| 7175320|               0|               11|        211|\n",
      "|154408915| 6893789|               3|              285|         95|\n",
      "+---------+--------+----------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|             user_id|          song_id| total_play_times| total_play_length|       song_length|\n",
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|               70629|            70629|            70629|             70629|             70629|\n",
      "|   mean|1.4711416415766895E8|7940891.432102961|69.03056818020926|20426.128233445186| 275.7063104390548|\n",
      "| stddev|2.9985219740135584E7|7414897.871879102|291.5746173380588| 98108.67151239264|340.99571065785665|\n",
      "|    min|              168955|             1672|                0|                 0|                 0|\n",
      "|    max|           154465966|         20876814|            27944|           8465072|              8946|\n",
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# based on total play times create ratings\n",
    "import math\n",
    "from pyspark.sql.functions import log\n",
    "data = data.withColumn('rating', when(data[\"total_play_times\"] >= 4, 5).otherwise((data[\"total_play_times\"]  ).cast('int') ) )\n",
    "#data = data.withColumn('rating', when(col('total_play_times') == 0, 0).otherwise(2*log(col('total_play_times')) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+-----------------+-----------+------+\n",
      "|  user_id| song_id|total_play_times|total_play_length|song_length|rating|\n",
      "+---------+--------+----------------+-----------------+-----------+------+\n",
      "|154420326|15208411|               0|               22|         59|     0|\n",
      "|154415167|  707143|               1|              228|        228|     1|\n",
      "|154420350| 6560373|               0|                1|        173|     0|\n",
      "|154422354| 7175320|               0|               11|        211|     0|\n",
      "|154408915| 6893789|               3|              285|         95|     3|\n",
      "+---------+--------+----------------+-----------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|             user_id|          song_id| total_play_times| total_play_length|       song_length|            rating|\n",
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|               70629|            70629|            70629|             70629|             70629|             70629|\n",
      "|   mean|1.4711416415766895E8|7940891.432102961|69.03056818020926|20426.128233445186| 275.7063104390548|0.9596766200852341|\n",
      "| stddev|2.9985219740135584E7|7414897.871879102|291.5746173380588| 98108.67151239264|340.99571065785665|1.3873531673183468|\n",
      "|    min|              168955|             1672|                0|                 0|                 0|                 0|\n",
      "|    max|           154465966|         20876814|            27944|           8465072|              8946|                 5|\n",
      "+-------+--------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recommender system ####\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_recommender = data.select('user_id', 'song_id', 'rating')\n",
    "matrix_recommender = matrix_recommender.filter('rating > 1')\n",
    "\n",
    "#del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|  user_id| song_id|rating|\n",
      "+---------+--------+------+\n",
      "|154408915| 6893789|     3|\n",
      "|154407321| 1244660|     5|\n",
      "|154422946|15807836|     5|\n",
      "|154422828| 4223040|     2|\n",
      "|154423513|20870988|     5|\n",
      "+---------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_recommender.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = matrix_recommender.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"user_id\", itemCol=\"song_id\", ratingCol=\"rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del(matrix_recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|             user_id|             song_id|            rating|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|                7520|                7520|              7520|\n",
      "|   mean|1.4818004088922873E8|1.0420211540558511E7| 4.052260638297873|\n",
      "| stddev|2.8016794676028725E7|   8231998.164158167|1.3560493129727433|\n",
      "|    min|              295224|                1756|                 2|\n",
      "|    max|           154465778|            20876481|                 5|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(matrix_recommender)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs = model.recommendForAllUsers(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  user_id|     recommendations|\n",
      "+---------+--------------------+\n",
      "|154463220|[[952877,5.786953...|\n",
      "|154425034|[[1217815,14.4940...|\n",
      "|154425484|[[6855432,4.70724...|\n",
      "|154425855|[[3602398,14.5501...|\n",
      "|154412037|[[7190414,4.93858...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|  user_id|Played_song|\n",
      "+---------+-----------+\n",
      "|154412037|         35|\n",
      "|154417891|         10|\n",
      "|154421973|         53|\n",
      "|154409364|          6|\n",
      "|154409339|         29|\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_song_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs.createOrReplaceTempView('user_recs')\n",
    "user_song_count.createOrReplaceTempView('user_song_count')\n",
    "\n",
    "user_recs = spark.sql(\"SELECT user_id, recommendations[0].song_id AS first_choice, recommendations[1].song_id AS second_choice, recommendations[2].song_id AS third_choice FROM user_recs\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------+------------+\n",
      "|  user_id|first_choice|second_choice|third_choice|\n",
      "+---------+------------+-------------+------------+\n",
      "|154463220|      952877|       203164|      703815|\n",
      "|154425034|     1217815|       952877|      203164|\n",
      "|154425484|     6855432|     11773482|     3849211|\n",
      "|154425855|     3602398|      6308715|     1217815|\n",
      "|154412037|     7190414|       280638|    15706616|\n",
      "+---------+------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs.createOrReplaceTempView('user_recs')\n",
    "user_recs = spark.sql(\"SELECT c.user_id, r.first_choice, r.second_choice, r.third_choice,  c.Played_song FROM user_recs r RIGHT JOIN user_song_count c ON r.user_id = c.user_id\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------+------------+-----------+\n",
      "|  user_id|first_choice|second_choice|third_choice|Played_song|\n",
      "+---------+------------+-------------+------------+-----------+\n",
      "| 10226023|        null|         null|        null|          2|\n",
      "|127185921|        null|         null|        null|         16|\n",
      "|139502126|        null|         null|        null|         17|\n",
      "|153952805|        null|         null|        null|         76|\n",
      "|154409339|       96217|      7107690|     3514084|         29|\n",
      "|154409364|        null|         null|        null|          6|\n",
      "|154412037|     7190414|       280638|    15706616|         35|\n",
      "|154417891|        null|         null|        null|         10|\n",
      "|154419409|        null|         null|        null|        106|\n",
      "|154421973|        null|         null|        null|         53|\n",
      "|154423790|        null|         null|        null|         16|\n",
      "|154424089|     6414906|      6635279|      650921|         12|\n",
      "|154424244|        null|         null|        null|         16|\n",
      "|154425034|     1217815|       952877|      203164|         11|\n",
      "|154425484|     6855432|     11773482|     3849211|          8|\n",
      "|154425500|        null|         null|        null|         14|\n",
      "|154425670|        null|         null|        null|         15|\n",
      "|154425745|        null|         null|        null|         16|\n",
      "|154425855|     3602398|      6308715|     1217815|         11|\n",
      "|154426030|        null|         null|        null|         15|\n",
      "|154426685|        null|         null|        null|          7|\n",
      "|154426687|        null|         null|        null|          7|\n",
      "|154434069|        null|         null|        null|          9|\n",
      "|154434111|        null|         null|        null|          8|\n",
      "|154434351|        null|         null|        null|         15|\n",
      "+---------+------------+-------------+------------+-----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_song1 = popular_songs.head(3)[0].song_id\n",
    "pop_song2 = popular_songs.head(3)[1].song_id\n",
    "pop_song3 = popular_songs.head(3)[2].song_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- first_choice: integer (nullable = true)\n",
      " |-- second_choice: integer (nullable = true)\n",
      " |-- third_choice: integer (nullable = true)\n",
      " |-- Played_song: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_recs.createOrReplaceTempView('user_recs')\n",
    "#user_recs = spark.sql(\"SELECT user_id,  (IFNULL(first_choice, pop_song1)) AS my_first_choice, (IFNULL(second_choice, pop_song2)) AS my_second_choice, (IFNULL(third_choice, pop_song3)) AS my_third_choice FROM user_recs\" )\n",
    "\n",
    "user_recs = user_recs.withColumn('first_rec', when(user_recs['first_choice'].isNull(), pop_song1).otherwise(user_recs['first_choice']))\n",
    "user_recs = user_recs.withColumn('second_rec', when(user_recs['second_choice'].isNull(), pop_song2).otherwise(user_recs['second_choice']))\n",
    "user_recs = user_recs.withColumn('third_rec', when(user_recs['third_choice'].isNull(), pop_song3).otherwise(user_recs['third_choice']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------+------------+-----------+---------+----------+---------+\n",
      "|  user_id|first_choice|second_choice|third_choice|Played_song|first_rec|second_rec|third_rec|\n",
      "+---------+------------+-------------+------------+-----------+---------+----------+---------+\n",
      "| 10226023|        null|         null|        null|          2| 20870989|  20870988| 20870987|\n",
      "|127185921|        null|         null|        null|         16| 20870989|  20870988| 20870987|\n",
      "|139502126|        null|         null|        null|         17| 20870989|  20870988| 20870987|\n",
      "|153952805|        null|         null|        null|         76| 20870989|  20870988| 20870987|\n",
      "|154409339|       96217|      7107690|     3514084|         29|    96217|   7107690|  3514084|\n",
      "|154409364|        null|         null|        null|          6| 20870989|  20870988| 20870987|\n",
      "|154412037|     7190414|       280638|    15706616|         35|  7190414|    280638| 15706616|\n",
      "|154417891|        null|         null|        null|         10| 20870989|  20870988| 20870987|\n",
      "|154419409|        null|         null|        null|        106| 20870989|  20870988| 20870987|\n",
      "|154421973|        null|         null|        null|         53| 20870989|  20870988| 20870987|\n",
      "+---------+------------+-------------+------------+-----------+---------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs = user_recs.select(user_recs['user_id'],\n",
    "                 user_recs['first_rec'],\n",
    "                 user_recs['second_rec'],\n",
    "                 user_recs['third_rec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+---------+\n",
      "|  user_id|first_rec|second_rec|third_rec|\n",
      "+---------+---------+----------+---------+\n",
      "| 10226023| 20870989|  20870988| 20870987|\n",
      "|127185921| 20870989|  20870988| 20870987|\n",
      "|139502126| 20870989|  20870988| 20870987|\n",
      "|153952805| 20870989|  20870988| 20870987|\n",
      "|154409339|    96217|   7107690|  3514084|\n",
      "|154409364| 20870989|  20870988| 20870987|\n",
      "|154412037|  7190414|    280638| 15706616|\n",
      "|154417891| 20870989|  20870988| 20870987|\n",
      "|154419409| 20870989|  20870988| 20870987|\n",
      "|154421973| 20870989|  20870988| 20870987|\n",
      "+---------+---------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| song_id|user_count|\n",
      "+--------+----------+\n",
      "|20870989|      1689|\n",
      "|20870988|      1628|\n",
      "|20870987|      1367|\n",
      "|20870990|      1302|\n",
      "|20870991|       973|\n",
      "+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popular_songs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20870987"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_songs.head(3)[2].song_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_recs = model.recommendForAllItems(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|song_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|  69042|[[1749320,0.0], [...|\n",
      "|  87724|[[1749320,0.0], [...|\n",
      "| 104656|[[1749320,0.0], [...|\n",
      "| 110904|[[154437261,2.845...|\n",
      "| 118989|[[154408665,3.138...|\n",
      "| 156365|[[154425546,5.087...|\n",
      "| 165310|[[1749320,0.0], [...|\n",
      "| 200878|[[154437692,6.948...|\n",
      "| 214739|[[1749320,0.0], [...|\n",
      "| 255362|[[154424853,4.771...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_recs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+------------+\n",
      "|  user_id|song_id|rating|  prediction|\n",
      "+---------+-------+------+------------+\n",
      "|  1685126|  69042|     0|         0.0|\n",
      "|154425373|  87724|     0|         0.0|\n",
      "|154419409| 110904|     0|0.0100495145|\n",
      "|154415157| 110904|     1|   1.0004137|\n",
      "|154460124| 118989|     1|   0.9813532|\n",
      "|154421730| 200878|     1|   0.9352329|\n",
      "|154425458| 255362|     0|         0.0|\n",
      "|154464207| 371006|     1|   0.9738073|\n",
      "|154420887| 403518|     0|         0.0|\n",
      "|154426419| 509175|     0|         0.0|\n",
      "|154449892| 534327|     1|   0.9562116|\n",
      "|154464695| 534327|     1|   0.9712486|\n",
      "|154460774| 534327|     1|   0.9915261|\n",
      "|154431425| 643423|     2|   1.9456455|\n",
      "|154435336| 643423|     0|  0.10103321|\n",
      "|154459435| 879366|     1|  0.97986615|\n",
      "|154426003|1029443|     1|  0.98263645|\n",
      "|  1685126|2301431|     0| 0.005492702|\n",
      "|154426268|3324609|     1|     0.97586|\n",
      "|154414800|3324609|     1|   1.0084307|\n",
      "+---------+-------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "0.0891132370161574\n"
     ]
    }
   ],
   "source": [
    "print('RMSE')\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_user = test.filter(test['user_id']==154425373).select(['user_id', 'song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|  user_id|song_id|\n",
      "+---------+-------+\n",
      "|154425373|4847647|\n",
      "|154425373|  87724|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = model.transform(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+\n",
      "|  user_id|song_id|prediction|\n",
      "+---------+-------+----------+\n",
      "|154425373|4847647|0.99685335|\n",
      "|154425373|  87724|       0.0|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.orderBy('Prediction', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|  user_id| song_id|count|\n",
      "+---------+--------+-----+\n",
      "|154420326|15208411|    1|\n",
      "|154415167|  707143|    1|\n",
      "|154420350| 6560373|    1|\n",
      "|154422354| 7175320|    1|\n",
      "|154408915| 6893789|    3|\n",
      "+---------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"user_id\", \"song_id\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = data.groupBy(\"user_id\", \"song_id\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|  user_id| song_id|count|\n",
      "+---------+--------+-----+\n",
      "|154420326|15208411|    1|\n",
      "|154415167|  707143|    1|\n",
      "|154420350| 6560373|    1|\n",
      "|154422354| 7175320|    1|\n",
      "|154408915| 6893789|    3|\n",
      "|154420736|  857978|    1|\n",
      "|154422927| 1068698|    1|\n",
      "|154407321| 1244660|   26|\n",
      "|154422946|15807836|    1|\n",
      "|154417195| 4652947|    1|\n",
      "|154413919| 3194852|    1|\n",
      "|154422346| 6253074|    2|\n",
      "|154422346| 1522598|    2|\n",
      "|154421525| 3534553|    1|\n",
      "|154416855|  996642|    1|\n",
      "|154414758| 7922570|    1|\n",
      "|154418894|11059349|    1|\n",
      "|154423104|  169744|    1|\n",
      "|154422828| 4223040|    2|\n",
      "|154422674|  158181|    1|\n",
      "+---------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|             song_id|             count|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|               27250|             27251|\n",
      "|   mean|   5033627.875853211|3.6695901067850722|\n",
      "| stddev|1.1072267282073868E7| 44.91230884564563|\n",
      "|    min|         -1649718862|                 1|\n",
      "|    max|            20876814|              5474|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"song_id\").count().describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_count = data.groupBy(\"song_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27251"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10069"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_count.filter(\"count > 1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------+\n",
      "| song_id|count|popular_song|\n",
      "+--------+-----+------------+\n",
      "|20869416|   94|          94|\n",
      "| 7112577|    6|           6|\n",
      "|  462334|    6|           6|\n",
      "| 2948915|    2|           2|\n",
      "| 7065988|    3|           3|\n",
      "+--------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_count.withColumn('popular_song',  song_count['count']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_count = song_count.withColumnRenamed('count', 'total_play_count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "| song_id|total_play_count|\n",
      "+--------+----------------+\n",
      "|20869416|              94|\n",
      "| 7112577|               6|\n",
      "|  462334|               6|\n",
      "| 2948915|               2|\n",
      "| 7065988|               3|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = data.groupBy('user_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  user_id|count|\n",
      "+---------+-----+\n",
      "|154412037|   35|\n",
      "|154417891|   10|\n",
      "|154421973|   56|\n",
      "|154409364|    6|\n",
      "|154409339|   29|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = user_count.withColumnRenamed('count', 'total_user_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|  user_id|total_user_count|\n",
      "+---------+----------------+\n",
      "|154412037|              35|\n",
      "|154417891|              10|\n",
      "|154421973|              56|\n",
      "|154409364|               6|\n",
      "|154409339|              29|\n",
      "+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.createOrReplaceTempView('data')\n",
    "user_count.createOrReplaceTempView('user_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine1 = spark.sql(\"SELECT data.user_id, data.song_id, data.count, user_count.total_user_count FROM data JOIN user_count ON data.user_id = user_count.user_id\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72039"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------------+\n",
      "|  user_id| song_id|count|total_user_count|\n",
      "+---------+--------+-----+----------------+\n",
      "|154420326|15208411|    1|              23|\n",
      "|154415167|  707143|    1|              15|\n",
      "|154420350| 6560373|    1|              96|\n",
      "|154422354| 7175320|    1|               3|\n",
      "|154408915| 6893789|    3|              46|\n",
      "+---------+--------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_count.createOrReplaceTempView('song_count')\n",
    "combine1.createOrReplaceTempView('combine1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine2 = spark.sql(\"SELECT combine1.*,  song_count.total_play_count FROM combine1 JOIN song_count ON combine1.song_id = song_count.song_id\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|             user_id|          song_id|             count|  total_user_count| total_play_count|\n",
      "+-------+--------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|               72004|            72004|             72004|             72004|            72004|\n",
      "|   mean|1.4702823311017445E8|7846242.112965946|1.3859646686295206|108.93175379145603|  320.11788233987|\n",
      "| stddev| 3.019756936019393E7| 9649417.46764295|3.3917304342656163| 380.1013408280859|772.9581538056328|\n",
      "|    min|                   0|      -1649718862|                 1|                 1|                1|\n",
      "|    max|           154465997|         20876814|               713|              2532|             5474|\n",
      "+-------+--------------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------------+----------------+\n",
      "|  user_id| song_id|count|total_user_count|total_play_count|\n",
      "+---------+--------+-----+----------------+----------------+\n",
      "|154420326|15208411|    1|              23|               1|\n",
      "|154415167|  707143|    1|              15|               1|\n",
      "|154420350| 6560373|    1|              96|               1|\n",
      "|154422354| 7175320|    1|               3|               1|\n",
      "|154408915| 6893789|    3|              46|              16|\n",
      "+---------+--------+-----+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine2 = combine2.withColumn('rating', when(combine2[\"count\"] > 4, 5).otherwise((combine2[\"count\"] ).cast('int') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------------+----------------+------+\n",
      "|  user_id| song_id|count|total_user_count|total_play_count|rating|\n",
      "+---------+--------+-----+----------------+----------------+------+\n",
      "|154420326|15208411|    1|              23|               1|     3|\n",
      "|154415167|  707143|    1|              15|               1|     3|\n",
      "|154420350| 6560373|    1|              96|               1|     3|\n",
      "|154422354| 7175320|    1|               3|               1|     3|\n",
      "|154408915| 6893789|    3|              46|              16|     5|\n",
      "+---------+--------+-----+----------------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- total_user_count: long (nullable = false)\n",
      " |-- total_play_count: long (nullable = false)\n",
      " |-- rating: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recommender system ####\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_recommender = combine2.select('user_id', 'song_id', 'rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+\n",
      "|summary|             user_id|          song_id|            rating|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "|  count|               72004|            72004|             72004|\n",
      "|   mean|1.4702823311017445E8|7846242.112965946|3.2295289150602744|\n",
      "| stddev| 3.019756936019393E7| 9649417.46764295|0.5833875522146803|\n",
      "|    min|                   0|      -1649718862|                 3|\n",
      "|    max|           154465997|         20876814|                 6|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_recommender.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = matrix_recommender.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-----------+\n",
      "|  user_id|song_id|rating| prediction|\n",
      "+---------+-------+------+-----------+\n",
      "|154460124| 118989|     3|-0.18767042|\n",
      "|154420071| 156365|     3|  2.1347246|\n",
      "|154458288| 156365|     3| 0.35034487|\n",
      "|154430537| 214739|     3|        NaN|\n",
      "|154437043| 255362|     3| 0.71764827|\n",
      "|154415183| 255362|     3|  3.3322303|\n",
      "| 37025504| 297391|     3|        NaN|\n",
      "|154448042| 304915|     3|        NaN|\n",
      "|154423858| 311014|     3|        NaN|\n",
      "|154446542| 327733|     3|  2.8554392|\n",
      "|154436860| 371006|     3|  0.9415884|\n",
      "|154417014| 455910|     3|        NaN|\n",
      "|154426419| 509175|     3|        NaN|\n",
      "|154464695| 534327|     3|  3.8092153|\n",
      "|154460944| 534327|     3| -2.7885003|\n",
      "|154464310| 534327|     3| 0.35848787|\n",
      "|154463366| 534327|     3|  2.9445949|\n",
      "|154463517| 534327|     3|  1.0452793|\n",
      "|154431425| 643423|     4| 0.46140617|\n",
      "|154422868| 643423|     3| -1.1077935|\n",
      "+---------+-------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"song_id\", ratingCol=\"rating\")\n",
    "model = als.fit(training)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
